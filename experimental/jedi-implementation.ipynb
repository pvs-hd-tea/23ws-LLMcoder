{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "type(completions): <class 'list'>\n",
      "--------------------\n",
      "Completion[0]: openai.resources.chat.completions.Completions.create\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, stream: Optional[Literal[False]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> ChatCompletion\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], stream: Literal[True], frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> Stream[ChatCompletionChunk]\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], stream: bool, frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> ChatCompletion | Stream[ChatCompletionChunk]\n",
      "--------------------\n",
      "Completion[1]: openai.resources.chat.completions.Completions.create\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, stream: Optional[Literal[False]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> ChatCompletion\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], stream: Literal[True], frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> Stream[ChatCompletionChunk]\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], stream: bool, frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> ChatCompletion | Stream[ChatCompletionChunk]\n",
      "--------------------\n",
      "Completion[2]: openai.resources.chat.completions.Completions.create\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, stream: Optional[Literal[False]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> ChatCompletion\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], stream: Literal[True], frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> Stream[ChatCompletionChunk]\n",
      "create(*, messages: List[ChatCompletionMessageParam], model: Union[\n",
      "            str,\n",
      "            Literal[\n",
      "                \"gpt-4-1106-preview\",\n",
      "                \"gpt-4-vision-preview\",\n",
      "                \"gpt-4\",\n",
      "                \"gpt-4-0314\",\n",
      "                \"gpt-4-0613\",\n",
      "                \"gpt-4-32k\",\n",
      "                \"gpt-4-32k-0314\",\n",
      "                \"gpt-4-32k-0613\",\n",
      "                \"gpt-3.5-turbo\",\n",
      "                \"gpt-3.5-turbo-16k\",\n",
      "                \"gpt-3.5-turbo-0301\",\n",
      "                \"gpt-3.5-turbo-0613\",\n",
      "                \"gpt-3.5-turbo-16k-0613\",\n",
      "            ],\n",
      "        ], stream: bool, frequency_penalty: Optional[float] | NotGiven=NOT_GIVEN, function_call: completion_create_params.FunctionCall | NotGiven=NOT_GIVEN, functions: List[completion_create_params.Function] | NotGiven=NOT_GIVEN, logit_bias: Optional[Dict[str, int]] | NotGiven=NOT_GIVEN, max_tokens: Optional[int] | NotGiven=NOT_GIVEN, n: Optional[int] | NotGiven=NOT_GIVEN, presence_penalty: Optional[float] | NotGiven=NOT_GIVEN, response_format: completion_create_params.ResponseFormat | NotGiven=NOT_GIVEN, seed: Optional[int] | NotGiven=NOT_GIVEN, stop: Union[Optional[str], List[str]] | NotGiven=NOT_GIVEN, temperature: Optional[float] | NotGiven=NOT_GIVEN, tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven=NOT_GIVEN, tools: List[ChatCompletionToolParam] | NotGiven=NOT_GIVEN, top_p: Optional[float] | NotGiven=NOT_GIVEN, user: str | NotGiven=NOT_GIVEN, extra_headers: Headers | None=None, extra_query: Query | None=None, extra_body: Body | None=None, timeout: float | httpx.Timeout | None | NotGiven=NOT_GIVEN) -> ChatCompletion | Stream[ChatCompletionChunk]\n",
      "--------------------\n",
      "Script: <Script: None <SameEnvironment: 3.11.6 in /home/kushal/.config/python.env/llmcoder.env>>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Generator\n",
    "\n",
    "import jedi\n",
    "from jedi.api.classes import Completion, Name, Signature\n",
    "from jedi.api.errors import SyntaxError\n",
    "\n",
    "\n",
    "class ResponseTypes:\n",
    "    List = list\n",
    "    Generator = Generator\n",
    "    Name = Name\n",
    "    Signature = Signature\n",
    "    Completion = Completion\n",
    "    SyntaxError = SyntaxError\n",
    "\n",
    "\n",
    "class AbstractResponse(ABC):\n",
    "    @abstractmethod\n",
    "    def print_response(self, completions: ResponseTypes.Name | ResponseTypes.Generator | ResponseTypes.List | ResponseTypes.Signature):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NameType(AbstractResponse):\n",
    "    def print_response(self, completions: ResponseTypes.Name):\n",
    "        print(completions)\n",
    "        print(f\"completions.full_name: {completions.full_name}\")\n",
    "        print(f\"completions.description: {completions.description}\")\n",
    "\n",
    "\n",
    "class GeneratorType(AbstractResponse):\n",
    "    def print_response(self, completions: ResponseTypes.Generator):\n",
    "        print(f\"CompleteSearch Generator: {completion}\")\n",
    "\n",
    "\n",
    "class ListType(AbstractResponse):\n",
    "    \"\"\" Completions / Responses Generator \"\"\"\n",
    "    def print_response(self, completions: ResponseTypes.List):\n",
    "        if len(completions) == 0:\n",
    "            return\n",
    "        for completion in completions:\n",
    "            yield completion\n",
    "\n",
    "\n",
    "class SignatureType(AbstractResponse):\n",
    "    def print_response(self, index, completion: ResponseTypes.Signature):\n",
    "        print(f\"Signatures[{index}]: {completion}\")\n",
    "\n",
    "\n",
    "# curr_py_env = sys.path\n",
    "# project_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)))\n",
    "# file_path = os.path.join(project_dir, \"src\", \"llmcoder\", \"analyze\", \"GPTReviewAnalyzer.py\")\n",
    "# curr_source_code = \"\"\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     curr_source_code = file.read()\n",
    "\n",
    "\n",
    "curr_source_code: str = \"\"\"import openai\n",
    "from llmcoder.utils import get_conversations_dir, get_openai_key, get_system_prompt, get_system_prompt_dir\n",
    "\n",
    "client = openai.OpenAI(api_key=get_openai_key())\n",
    "client.chat.completions.create(messages=self.messages, model=model, temperature=temperature, n=n)\n",
    "\"\"\"\n",
    "# source_lines: list[str] = source.splitlines()\n",
    "source_line: int = 82\n",
    "source_line: int = 5\n",
    "# source_col: int = 42 # .chat : instance\n",
    "source_col: int = 62 # create() : def\n",
    "source_col: int = 31 # create() : def\n",
    "\n",
    "# from pipreqs import pipreqs\n",
    "\n",
    "# pipreqs.project_dir = project_dir\n",
    "# import subprocess\n",
    "\n",
    "# subprocess.run([\"pipreqs\", \"--print\", project_dir])\n",
    "# jedi.preload_module([\"os\", \"re\", \"openai\", \"llmcoder\"])\n",
    "# project = jedi.Project(path=project_dir, environment_path=\"/home/kushal/.config/python.env/llmcoder.env\")\n",
    "# script = jedi.Script(curr_source_code, path=file_path, project=project, environment=project.get_environment())\n",
    "signatures = []\n",
    "# script = jedi.Script(curr_source_code, path=file_path)\n",
    "script = jedi.Script(curr_source_code)\n",
    "# completions = script.complete_search()\n",
    "# completions = script.infer(source_line, source_col)\n",
    "completions = script.get_signatures(source_line, source_col)\n",
    "# completions = script.goto(source_line, source_col)\n",
    "# completions = script.infer()\n",
    "# completions = script.get_references()\n",
    "# completions = script.complete(source_line, source_col)\n",
    "# completions = script.get_context(source_line, source_col)\n",
    "# completions = script.get_syntax_errors()\n",
    "# completions = script.extract_function()\n",
    "# completions = script.get_names()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"type(completions): {type(completions)}\")\n",
    "print(\"-\"*20)\n",
    "if isinstance(completions, ResponseTypes.Name):\n",
    "    # Get context\n",
    "    print(f\"Get context: {completions}\")\n",
    "    print(\"-\"*20)\n",
    "elif isinstance(completions, ResponseTypes.List):\n",
    "    if len(completions) == 0:\n",
    "        print(\"Empty Responses\")\n",
    "        print(\"-\"*20)\n",
    "    for index, completion in enumerate(completions):\n",
    "        if isinstance(completion, ResponseTypes.Name):\n",
    "            # Get references\n",
    "            print(f\"Completion[{index}]: {completion.full_name}\")\n",
    "            print(f\"{completion._get_docstring_signature()}\")\n",
    "            if hasattr(completion, completion._get_docstring_signature()) and \"def\" in completion.description or \"class\" in completion.description:\n",
    "                print(f\"Completion[{index}]._get_docstring_signature: {completion._get_docstring_signature()}\")\n",
    "                # goto_definitions = completion.goto(follow_imports=True, follow_builtin_imports=True)\n",
    "                # print(f\"Completion[{index}].get_line_code: {completion.get_line_code(after=completion.get_definition_end_position()[0]-completion.get_definition_start_position()[0])}\")\n",
    "                # print(f\"Completions[{index}].goto.get_line_code: {goto_definitions[0].get_line_code(after=goto_definitions[0].get_definition_end_position()[0]-goto_definitions[0].get_definition_start_position()[0])}\")\n",
    "            print(\"-\"*20)\n",
    "        elif isinstance(completion, ResponseTypes.Signature):\n",
    "            # Signatures\n",
    "            print(f\"Signatures[{index}]: {completion.docstring()}\")\n",
    "            print(\"-\"*20)\n",
    "        elif isinstance(completion, ResponseTypes.Generator):\n",
    "            # Complete search\n",
    "            print(f\"Complete Search[{index}]: {next(completion)}\")\n",
    "            print(\"-\"*20)\n",
    "        elif isinstance(completion, ResponseTypes.SyntaxError):\n",
    "            # Syntax errors\n",
    "            print(f\"SyntaxErrors[{index}]: {completion}\")\n",
    "            print(f\"Error line: {completion.line}\")\n",
    "            print(f\"Error column: {completion.column}\")\n",
    "            print(f\"Error until line: {completion.until_line}\")\n",
    "            print(f\"Error until column: {completion.until_column}\")\n",
    "            print(f\"Error message: {completion.get_message()}\")\n",
    "            print(\"-\"*20)\n",
    "        elif isinstance(completion, ResponseTypes.Completion):\n",
    "            # Get completion suggestions\n",
    "            print(f\"Completions[{index}]: {completion}\")\n",
    "            print(f\"Completions[{index}].complete: {completion.complete}\")\n",
    "            print(f\"Completions[{index}].name: {completion.name}\")\n",
    "            print(\"-\"*20)\n",
    "        else:\n",
    "            print(f\"Completions[{index}]: {completion}\")\n",
    "            print(f\"Completions[{index}].complete: {completion.complete}\")\n",
    "            print(f\"Completions[{index}].name: {completion.name}\")\n",
    "            print(\"-\"*20)\n",
    "\n",
    "print(f\"Script: {script}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
